{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML By Example :: Time Series Data \n",
    "\n",
    "In this tutorial we will perform following tasks:  \n",
    "- [ ] Download Stock Data to [GCS](https://cloud.google.com/storage/docs)\n",
    "- [ ] Import the stock data into [BigQuery](https://cloud.google.com/bigquery/docs)\n",
    "- [ ] Load the stock data from files in GCS into a data frame\n",
    "\n",
    "# References\n",
    "1. https://github.com/GoogleCloudPlatform/training-data-analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import google.cloud.bigquery as bq\n",
    "import google.cloud.storage as gcs\n",
    "import yfinance as yf\n",
    "from datetime import date\n",
    "import os\n",
    "import numpy as np\n",
    "from googleapiclient.errors import HttpError\n",
    "from google.cloud import exceptions\n",
    "import glob\n",
    "import shutil\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'inspired-campus-278503'\n",
    "PROJECT = 'inspired-campus-278503'\n",
    "REGION = 'us-central1'\n",
    "DATASET_NAME = \"kl_ml_by_example\"\n",
    "DATASET_LOCATION = REGION\n",
    "DATASET_TABLE = \"stockdata\"\n",
    "\n",
    "LOCAL_TEMP_DIR = \"./tmp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['DATASET_LOCATION'] = DATASET_LOCATION\n",
    "os.environ['DATASET_NAME'] = DATASET_NAME\n",
    "os.environ['DATASET_TABLE'] = DATASET_TABLE\n",
    "\n",
    "os.environ['LOCAL_TEMP_DIR'] = LOCAL_TEMP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the storage bucket if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gcs_client():\n",
    "    return gcs.Client()\n",
    "\n",
    "def get_bq_client():\n",
    "    return bq.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_storage_bucket(bucket_name=None):\n",
    "    if bucket_name is None:\n",
    "        bucket_name = BUCKET\n",
    "    gcs_client = get_gcs_client()\n",
    "    gcs_bucket = None\n",
    "    try:\n",
    "        # Get the bucket\n",
    "        gcs_bucket = gcs_client.get_bucket(bucket_name)\n",
    "    except exceptions.NotFound as not_found_err:\n",
    "        gcs_client.create_bucket(bucket_name, location=DATASET_LOCATION)\n",
    "        print(\"Created bucket: {bucket}!\".format(bucket=bucket_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bq_dataset(ds_name=None):\n",
    "    if ds_name is None:\n",
    "        ds_name = DATASET_NAME\n",
    "    bq_client = get_bq_client()\n",
    "    bq_client.create_dataset(ds_name, exists_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_files_to_storage(fully_qualified_file_pattern, bucket=None):\n",
    "    file_names = glob.glob(fully_qualified_file_pattern)\n",
    "    if bucket is None:\n",
    "        bucket = BUCKET\n",
    "    gcs_client = get_gcs_client()\n",
    "    gcs_bucket = gcs_client.bucket(bucket)\n",
    "    for file_name in file_names:\n",
    "        fname = file_name.split(\"/\")[-1]\n",
    "        blob = gcs_bucket.blob(fname)\n",
    "        blob.upload_from_filename(file_name)\n",
    "        print(\"Copying '{fname}' to '{bucket}'\".format(fname=fname, bucket=bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_historical_data(stocks=[\"MSFT\"], start_date=\"2000-01-01\", end_date=None):\n",
    "    if end_date is None:\n",
    "        end_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "    tickers = \"\"\n",
    "    for ticker in stocks:\n",
    "        tickers = tickers + \" \" + ticker\n",
    "    data = yf.download(\n",
    "        tickers=tickers,\n",
    "        start=start_date,\n",
    "        end=end_date,\n",
    "        group_by=\"ticker\"\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  3 of 3 completed\n"
     ]
    }
   ],
   "source": [
    "stock_list = [\"MSFT\", \"GOOG\", \"FB\"]\n",
    "data = get_stock_historical_data(stock_list)\n",
    "#data = get_stock_historical_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def offset_and_copy(column, start_from_row=0):\n",
    "    new_col = np.array(column[start_from_row:-1])\n",
    "    for i in range(start_from_row+1):\n",
    "        new_col = np.append(new_col,[np.NaN])\n",
    "    return new_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process DataFrame\n",
    "# 1. Creates a deep copy of the input data frame\n",
    "# 2. Cleanses the data frame\n",
    "# 3. Returns the new data frame\n",
    "def pre_process(df):\n",
    "    data_frame = df.copy(deep=True)\n",
    "    \n",
    "    #print(data_frame)\n",
    "    #print(\"Finite? \", np.isfinite(data_frame[\"Volume\"]))\n",
    "    data_frame[\"Volume\"] = data_frame[\"Volume\"].astype(int)\n",
    "    for count in range(1, 5):\n",
    "        data_frame[\"Next_High_\" + str(count)] = offset_and_copy(np.array(data_frame[\"High\"]), count)\n",
    "        data_frame[\"Next_Low_\" + str(count)] = offset_and_copy(np.array(data_frame[\"Low\"]), count)\n",
    "    data_frame = data_frame.dropna()\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create a tmp directory if one doesn't exist\n",
    "mkdir -p ${LOCAL_TEMP_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock MSFT has 5136 records.\n",
      "Stock GOOG has 3974 records.\n",
      "Stock FB has 2022 records.\n"
     ]
    }
   ],
   "source": [
    "for stock in stock_list:\n",
    "    df = pre_process(data[stock].dropna())\n",
    "    #print(df)\n",
    "    dates = np.array(np.array(np.array(df.index.to_numpy(), dtype=\"str\"), dtype=\"S10\"), dtype=\"str\")\n",
    "    fname = \"{local_temp_dir}/{stock}-{start_date}-TO-{end_date}.csv\".format(local_temp_dir=LOCAL_TEMP_DIR,stock=stock, start_date=dates[0], end_date=dates[-1])\n",
    "    df[\"Stock_Ticker\"]=stock\n",
    "    df.to_csv(fname, index=True)\n",
    "    print(\"Stock {stock} has {records} records.\".format(stock=stock, records=len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created bucket: inspired-campus-278503!\n",
      "Copying 'FB-2012-05-18-TO-2020-06-02.csv' to 'inspired-campus-278503'\n",
      "Copying 'GOOG-2004-08-19-TO-2020-06-02.csv' to 'inspired-campus-278503'\n",
      "Copying 'MSFT-2000-01-03-TO-2020-06-02.csv' to 'inspired-campus-278503'\n"
     ]
    }
   ],
   "source": [
    "create_storage_bucket()\n",
    "create_bq_dataset()\n",
    "upload_files_to_storage(LOCAL_TEMP_DIR + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_into_bq(gcs_uris=None):\n",
    "    files = list()\n",
    "    if gcs_uris is None:\n",
    "        blobs = get_gcs_client().list_blobs(BUCKET)\n",
    "        for blob in blobs:\n",
    "            print(\"blob: \", blob.name)\n",
    "            files.append(\"gs://{bucket}/{blob}\".format(bucket=BUCKET, blob=blob.name))\n",
    "    else:\n",
    "        files = gcs_uris\n",
    "    table_name = \"{project}.{dataset}.{table}\".format(\n",
    "        project = PROJECT, \n",
    "        dataset = DATASET_NAME, \n",
    "        table = DATASET_TABLE\n",
    "        )\n",
    "    bq_client = get_bq_client()\n",
    "    job_config = bq.LoadJobConfig()\n",
    "    job_config.autodetect = True\n",
    "    job_config.source_format = bq.SourceFormat.CSV\n",
    "    print(files)\n",
    "    load_job = bq_client.load_table_from_uri(files, bq_client.dataset(DATASET_NAME).table(DATASET_TABLE), job_config=job_config)\n",
    "    print(\"Starting job {}\".format(load_job.job_id))\n",
    "    try:\n",
    "        load_job.result()  # Waits for table load to complete.\n",
    "        print(\"Job finished.\")\n",
    "    except exceptions.BadRequest as err:\n",
    "        print(load_job.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blob:  FB-2012-05-18-TO-2020-06-02.csv\n",
      "blob:  GOOG-2004-08-19-TO-2020-06-02.csv\n",
      "blob:  MSFT-2000-01-03-TO-2020-06-02.csv\n",
      "['gs://inspired-campus-278503/FB-2012-05-18-TO-2020-06-02.csv', 'gs://inspired-campus-278503/GOOG-2004-08-19-TO-2020-06-02.csv', 'gs://inspired-campus-278503/MSFT-2000-01-03-TO-2020-06-02.csv']\n",
      "Starting job dc46c2d2-9a81-4c91-b59c-be3d04baaab0\n",
      "Job finished.\n"
     ]
    }
   ],
   "source": [
    "load_data_into_bq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    # Delete the local temp directory and its contents\n",
    "    try:\n",
    "        shutil.rmtree(LOCAL_TEMP_DIR)\n",
    "    except OSError as e:\n",
    "        print(\"[{path}] doesn't exist. Ignoring FileNotFoundError!\".format(path=LOCAL_TEMP_DIR))\n",
    "    # Delete the contents on GCS bucket\n",
    "    try:\n",
    "        get_gcs_client().bucket(BUCKET).delete(force=True)\n",
    "        print(\"Bucket {bucket} deleted\".format(bucket=BUCKET))\n",
    "    except exceptions.NotFound as e:\n",
    "        # pass\n",
    "        print(\"Bucket {bucket} doesn't exist\".format(bucket=BUCKET))\n",
    "    # Remove the BQ Table\n",
    "    table_id = '{project}.{dataset}.{table}'.format(project=PROJECT, dataset=DATASET_NAME, table=DATASET_TABLE)\n",
    "    bq.Client().delete_table(table_id, not_found_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_ml_models(ml_models=None):\n",
    "    bq = get_bq_client()\n",
    "    if ml_models is None:\n",
    "        ml_models = bq.list_models(\"{project}.{dataset}\".format(project=PROJECT, dataset=DATASET_NAME))\n",
    "    for ml_model in ml_models:\n",
    "        bq.delete_model(ml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleanup()\n",
    "#cleanup_ml_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_to_split_data(stock, dataset_split_percentage=75, random_sample_percentage=30):\n",
    "    COUNT_SQL = \"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) AS REC_COUNT\n",
    "    FROM \n",
    "        `{project}.{dataset}.{table}`\n",
    "    WHERE\n",
    "        Stock_Ticker='{stock}'\n",
    "    \"\"\".format(project=PROJECT, dataset=DATASET_NAME, table=DATASET_TABLE, stock=stock)\n",
    "    bq = get_bq_client();\n",
    "    df_record_count = bq.query(COUNT_SQL).to_dataframe()\n",
    "    record_count = int(df_record_count.iat[0,0])\n",
    "    split_count = int(record_count*dataset_split_percentage/100)\n",
    "    random_sample_count = float(random_sample_percentage/100)\n",
    "    print(\"Total number of records for {stock} is: {record_count}\".format(stock=stock, record_count=record_count))\n",
    "    print(\"Number of training records is: {split_count}\".format(split_count=split_count))\n",
    "    SQL = \"\"\"\n",
    "    #standardSQL\n",
    "    SELECT \n",
    "        Open,\n",
    "        Close,\n",
    "        Low,\n",
    "        HASH_FUNC,\n",
    "        High,\n",
    "        Next_High_1,\n",
    "        Next_Low_1,\n",
    "        Next_High_2,\n",
    "        Next_Low_2,\n",
    "        Next_High_3,\n",
    "        Next_Low_3,\n",
    "        Next_High_4,\n",
    "        Next_Low_4\n",
    "    FROM (\n",
    "        SELECT\n",
    "            #COUNT(*) AS Num,\n",
    "            Open,\n",
    "            Close,\n",
    "            High,\n",
    "            Low,\n",
    "            Next_High_1,\n",
    "            Next_Low_1,\n",
    "            Next_High_2,\n",
    "            Next_Low_2,\n",
    "            Next_High_3,\n",
    "            Next_Low_3,\n",
    "            Next_High_4,\n",
    "            Next_Low_4,\n",
    "            FARM_FINGERPRINT(CONCAT(\n",
    "                CAST(Date As STRING),\n",
    "                CAST(Stock_Ticker As STRING)\n",
    "            )) AS HASH_FUNC\n",
    "        FROM\n",
    "            `{project}.{dataset}.{table}`\n",
    "        WHERE\n",
    "            Stock_Ticker='{stock}'\n",
    "    )\n",
    "    WHERE \n",
    "        (ABS(MOD(HASH_FUNC, 10)) {{less_or_greater_than_operator}} ({dataset_split_percentage}/10)) #split the dataset to the requested percentage\n",
    "        AND \n",
    "        (RAND() < ({random_sample_count}))\n",
    "    \"\"\".format(project=PROJECT, dataset=DATASET_NAME, table=DATASET_TABLE, stock=stock,dataset_split_percentage=dataset_split_percentage,random_sample_count=random_sample_count)\n",
    "    \n",
    "    train_df = bq.query(SQL.format(less_or_greater_than_operator=\"<\")).to_dataframe()\n",
    "    eval_df = bq.query(SQL.format(less_or_greater_than_operator=\">\")).to_dataframe()\n",
    "    return SQL, train_df, eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ml_model(sql, stock, less_or_greater_than_operator=\"<\"):\n",
    "    sql = sql.format(less_or_greater_than_operator=less_or_greater_than_operator)\n",
    "    SQL=\"\"\"\n",
    "    #standardSQL\n",
    "    CREATE MODEL `{project}.{dataset}.{model}`\n",
    "    OPTIONS\n",
    "    (model_type='linear_reg',\n",
    "     input_label_cols=[\n",
    "       'Next_High_1']\n",
    "       # Multiple lables not supported in BQ\n",
    "       #'Next_High_1',\n",
    "       # 'Next_Low_1',\n",
    "       # 'Next_High_2',\n",
    "       # 'Next_Low_2',\n",
    "       # 'Next_High_3',\n",
    "       # 'Next_Low_3',\n",
    "       # 'Next_High_4',\n",
    "       # 'Next_Low_4']\n",
    "    ) AS\n",
    "    {sql}\n",
    "    \"\"\".format(project=PROJECT, dataset=DATASET_NAME, model=stock+\"_ml_model\",sql=sql)\n",
    "    print(SQL)\n",
    "    create_model_job = get_bq_client().query(SQL)\n",
    "    print(\"Starting create ml model job: {}\".format(create_model_job.job_id))\n",
    "    try:\n",
    "        create_model_job.result()  # Waits for table load to complete.\n",
    "        print(\"Finished creating ML model.\")\n",
    "    except exceptions.BadRequest as err:\n",
    "        print(\"Error\", create_model_job.errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of records for MSFT is: 5136\n",
      "Number of training records is: 3852\n",
      "Training record#:  1252\n",
      "Evaluation record#:  331\n",
      "\n",
      "    #standardSQL\n",
      "    CREATE MODEL `inspired-campus-278503.kl_ml_by_example.MSFT_ml_model`\n",
      "    OPTIONS\n",
      "    (model_type='linear_reg',\n",
      "     input_label_cols=[\n",
      "       'Next_High_1',\n",
      "        'Next_Low_1',\n",
      "        'Next_High_2',\n",
      "        'Next_Low_2',\n",
      "        'Next_High_3',\n",
      "        'Next_Low_3',\n",
      "        'Next_High_4',\n",
      "        'Next_Low_4']\n",
      "    ) AS\n",
      "    \n",
      "    #standardSQL\n",
      "    SELECT \n",
      "        Open,\n",
      "        Close,\n",
      "        Low,\n",
      "        HASH_FUNC,\n",
      "        High,\n",
      "        Next_High_1,\n",
      "        Next_Low_1,\n",
      "        Next_High_2,\n",
      "        Next_Low_2,\n",
      "        Next_High_3,\n",
      "        Next_Low_3,\n",
      "        Next_High_4,\n",
      "        Next_Low_4\n",
      "    FROM (\n",
      "        SELECT\n",
      "            #COUNT(*) AS Num,\n",
      "            Open,\n",
      "            Close,\n",
      "            High,\n",
      "            Low,\n",
      "            Next_High_1,\n",
      "            Next_Low_1,\n",
      "            Next_High_2,\n",
      "            Next_Low_2,\n",
      "            Next_High_3,\n",
      "            Next_Low_3,\n",
      "            Next_High_4,\n",
      "            Next_Low_4,\n",
      "            FARM_FINGERPRINT(CONCAT(\n",
      "                CAST(Date As STRING),\n",
      "                CAST(Stock_Ticker As STRING)\n",
      "            )) AS HASH_FUNC\n",
      "        FROM\n",
      "            `inspired-campus-278503.kl_ml_by_example.stockdata`\n",
      "        WHERE\n",
      "            Stock_Ticker='MSFT'\n",
      "    )\n",
      "    WHERE \n",
      "        (ABS(MOD(HASH_FUNC, 10)) < (75/10)) #split the dataset to the requested percentage\n",
      "        AND \n",
      "        (RAND() < (0.3))\n",
      "    \n",
      "    \n",
      "Starting create ml model job: 5dbae9ac-d97a-4a13-8348-f04800c7ab85\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "stock = \"MSFT\"\n",
    "sql, train_df, eval_df = create_sql_to_split_data(stock)\n",
    "print(\"Training record#: \", len(train_df))\n",
    "print(\"Evaluation record#: \", len(eval_df))\n",
    "#print(\"SQL: \", sql)\n",
    "create_ml_model(sql, stock)\n",
    "#print (sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
